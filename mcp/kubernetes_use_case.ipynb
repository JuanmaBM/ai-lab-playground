{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1363d0a4",
   "metadata": {},
   "source": [
    "## Kubernetes Agent\n",
    "\n",
    "Also, the instructions are changes from [basic use case](./basic_tool_use_case.ipynb), and now we are force to the LLM to use kube_client when some Kubernetes keyword appears in the prompt. Take a look to these instructions and take a time to understand the implications they can have to the LLM.\n",
    "\n",
    "We’re going to follow the same approach as in the basic tool use case, using Llama Stack and MCP. However, this time, we’ll explore a more realistic scenario. We’ve implemented a Kubernetes client in [kube_client.py](./tools/kube_client.py), which allows us to connect to a Kubernetes cluster and execute commands.\n",
    "\n",
    "To simplify the use case, we’ll use a local Minikube instance, which we are already logged into.\n",
    "\n",
    "Additionally, the model instructions differ from those in the basic use case. In this case, we explicitly require the LLM to use kube_client whenever Kubernetes-related keywords appear in the prompt.\n",
    "Take a moment to review these instructions and understand the implications they may have on the model’s behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbarea/Documents/hcm/ai/ai-lab-playground/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">ollama</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mollama\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "container_image: null\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "external_providers_dir: null\n",
       "image_name: ollama\n",
       "inference_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">inference_store.db</span>\n",
       "  type: sqlite\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: llama3.<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:3b</span>-instruct-fp16\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "      responses_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">responses_store.db</span>\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">huggingface_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">localfs_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meta_reference_eval.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:11434</span>\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  post_training:\n",
       "  - config:\n",
       "      checkpoint_format: huggingface\n",
       "      device: cpu\n",
       "      distributed_backend: null\n",
       "    provider_id: huggingface\n",
       "    provider_type: inline::huggingface\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: <span style=\"font-weight: bold\">[]</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  auth: null\n",
       "  host: null\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: <span style=\"font-weight: bold\">[]</span>\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "container_image: null\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "external_providers_dir: null\n",
       "image_name: ollama\n",
       "inference_store:\n",
       "  db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95minference_store.db\u001b[0m\n",
       "  type: sqlite\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: llama3.\u001b[1;92m2:3b\u001b[0m-instruct-fp16\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "      responses_store:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mresponses_store.db\u001b[0m\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mhuggingface_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mlocalfs_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mmeta_reference_eval.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: \u001b[4;94mhttp://localhost:11434\u001b[0m\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  post_training:\n",
       "  - config:\n",
       "      checkpoint_format: huggingface\n",
       "      device: cpu\n",
       "      distributed_backend: null\n",
       "    provider_id: huggingface\n",
       "    provider_type: inline::huggingface\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: \u001b[32m''\u001b[0m\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  auth: null\n",
       "  host: null\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "from llama_stack_client.types.toolgroup_register_params import McpEndpoint\n",
    "\n",
    "\n",
    "model = \"llama3.2:3b-instruct-fp16\"\n",
    "os.environ[\"INFERENCE_MODEL\"] = model\n",
    "client = LlamaStackAsLibraryClient('ollama')\n",
    "client.initialize()\n",
    "\n",
    "\n",
    "client.toolgroups.register(\n",
    "    toolgroup_id=\"mcp::custom\",\n",
    "    provider_id=\"model-context-protocol\",\n",
    "    mcp_endpoint=McpEndpoint(uri=\"http://127.0.0.1:5000/sse\"),\n",
    ")\n",
    "\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a smart assistant that helps users with his questions.\n",
    "You have differents available tools and you must use to retrieve information or perform actions.\n",
    "You must not generate code or scripts.\n",
    "No simulate the output, perform all operations with tools available.\n",
    "You are force to use kube_command tool to create resources in kubernetes, so if the user ask create something you MUST use the tool.\n",
    "Not use yaml to create resources, Just use kubectl commands\n",
    "You have to add kubectl in the commands you pass to kubec_command tool, for instance kubectl get pods \n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "  client,\n",
    "  model=model,\n",
    "  instructions=instructions,\n",
    "  tools=[\"mcp::custom\"],\n",
    "  sampling_params={\n",
    "    \"strategy\": {\"type\": \"top_p\", \"temperature\": 0.2, \"top_p\": 0.9},\n",
    "  },\n",
    ")\n",
    "session_id = agent.create_session('new_session')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d77e6",
   "metadata": {},
   "source": [
    "We can ask specify what exactly we need to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020f7772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Use the kube_command to execute `kubectl get pods -A`. Show me the output\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mube\u001b[0m\u001b[33m_command\u001b[0m\u001b[33m(cmd\u001b[0m\u001b[33m='\u001b[0m\u001b[33mkubectl\u001b[0m\u001b[33m get\u001b[0m\u001b[33m pods\u001b[0m\u001b[33m -\u001b[0m\u001b[33mA\u001b[0m\u001b[33m',\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m='\u001b[0m\u001b[33msession\u001b[0m\u001b[33m_\u001b[0m\u001b[33m123\u001b[0m\u001b[33m')]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Args:{'cmd': 'kubectl get pods -A', 'session_id': 'session_123'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": \"NAMESPACE     NAME                               READY   STATUS    RESTARTS         AGE\\\\nkube-system   coredns-6f6b679f8f-6z4gj           1/1     Running   13 (86d ago)     181d\\\\nkube-system   etcd-minikube                      1/1     Running   13 (86d ago)     181d\\\\nkube-system   kindnet-6jvrc                      1/1     Running   0                181d\\\\nkube-system   kindnet-jmlkf                      1/1     Running   14 (86d ago)     181d\\\\nkube-system   kindnet-k4g5p                      1/1     Running   0                181d\\\\nkube-system   kube-apiserver-minikube            1/1     Running   13 (86d ago)     181d\\\\nkube-system   kube-controller-manager-minikube   1/1     Running   30 (86d ago)     181d\\\\nkube-system   kube-proxy-cvgc5                   1/1     Running   13 (86d ago)     181d\\\\nkube-system   kube-proxy-pb588                   1/1     Running   0                181d\\\\nkube-system   kube-proxy-zqj4d                   1/1     Running   0                181d\\\\nkube-system   kube-scheduler-minikube            1/1     Running   13 (86d ago)     181d\\\\nkube-system   storage-provisioner                1/1     Running   33 (3h47m ago)   181d\\\\n\"\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m output\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m command\u001b[0m\u001b[33m `\u001b[0m\u001b[33mkubectl\u001b[0m\u001b[33m get\u001b[0m\u001b[33m pods\u001b[0m\u001b[33m -\u001b[0m\u001b[33mA\u001b[0m\u001b[33m`\u001b[0m\u001b[33m is\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m``\u001b[0m\u001b[33m`\n",
      "\u001b[0m\u001b[33mNAMESPACE\u001b[0m\u001b[33m    \u001b[0m\u001b[33m NAME\u001b[0m\u001b[33m                              \u001b[0m\u001b[33m READY\u001b[0m\u001b[33m  \u001b[0m\u001b[33m STATUS\u001b[0m\u001b[33m   \u001b[0m\u001b[33m RE\u001b[0m\u001b[33mSTART\u001b[0m\u001b[33mS\u001b[0m\u001b[33m        \u001b[0m\u001b[33m AGE\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m c\u001b[0m\u001b[33mored\u001b[0m\u001b[33mns\u001b[0m\u001b[33m-\u001b[0m\u001b[33m6\u001b[0m\u001b[33mf\u001b[0m\u001b[33m6\u001b[0m\u001b[33mb\u001b[0m\u001b[33m679\u001b[0m\u001b[33mf\u001b[0m\u001b[33m8\u001b[0m\u001b[33mf\u001b[0m\u001b[33m-\u001b[0m\u001b[33m6\u001b[0m\u001b[33mz\u001b[0m\u001b[33m4\u001b[0m\u001b[33mg\u001b[0m\u001b[33mj\u001b[0m\u001b[33m          \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m13\u001b[0m\u001b[33m (\u001b[0m\u001b[33m86\u001b[0m\u001b[33md\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m et\u001b[0m\u001b[33mcd\u001b[0m\u001b[33m-min\u001b[0m\u001b[33mik\u001b[0m\u001b[33mube\u001b[0m\u001b[33m                     \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m13\u001b[0m\u001b[33m (\u001b[0m\u001b[33m86\u001b[0m\u001b[33md\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kind\u001b[0m\u001b[33mnet\u001b[0m\u001b[33m-\u001b[0m\u001b[33m6\u001b[0m\u001b[33mj\u001b[0m\u001b[33mv\u001b[0m\u001b[33mrc\u001b[0m\u001b[33m                     \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m0\u001b[0m\u001b[33m               \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kind\u001b[0m\u001b[33mnet\u001b[0m\u001b[33m-j\u001b[0m\u001b[33mml\u001b[0m\u001b[33mkf\u001b[0m\u001b[33m                     \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m14\u001b[0m\u001b[33m (\u001b[0m\u001b[33m86\u001b[0m\u001b[33md\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kind\u001b[0m\u001b[33mnet\u001b[0m\u001b[33m-k\u001b[0m\u001b[33m4\u001b[0m\u001b[33mg\u001b[0m\u001b[33m5\u001b[0m\u001b[33mp\u001b[0m\u001b[33m                     \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m0\u001b[0m\u001b[33m               \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kube\u001b[0m\u001b[33m-\u001b[0m\u001b[33mapis\u001b[0m\u001b[33merver\u001b[0m\u001b[33m-min\u001b[0m\u001b[33mik\u001b[0m\u001b[33mube\u001b[0m\u001b[33m           \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m13\u001b[0m\u001b[33m (\u001b[0m\u001b[33m86\u001b[0m\u001b[33md\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kube\u001b[0m\u001b[33m-controller\u001b[0m\u001b[33m-manager\u001b[0m\u001b[33m-min\u001b[0m\u001b[33mik\u001b[0m\u001b[33mube\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m30\u001b[0m\u001b[33m (\u001b[0m\u001b[33m86\u001b[0m\u001b[33md\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kube\u001b[0m\u001b[33m-proxy\u001b[0m\u001b[33m-c\u001b[0m\u001b[33mvg\u001b[0m\u001b[33mc\u001b[0m\u001b[33m5\u001b[0m\u001b[33m                  \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m13\u001b[0m\u001b[33m (\u001b[0m\u001b[33m86\u001b[0m\u001b[33md\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kube\u001b[0m\u001b[33m-proxy\u001b[0m\u001b[33m-p\u001b[0m\u001b[33mb\u001b[0m\u001b[33m588\u001b[0m\u001b[33m                  \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m0\u001b[0m\u001b[33m               \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kube\u001b[0m\u001b[33m-proxy\u001b[0m\u001b[33m-z\u001b[0m\u001b[33mq\u001b[0m\u001b[33mj\u001b[0m\u001b[33m4\u001b[0m\u001b[33md\u001b[0m\u001b[33m                  \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m0\u001b[0m\u001b[33m               \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m kube\u001b[0m\u001b[33m-s\u001b[0m\u001b[33mcheduler\u001b[0m\u001b[33m-min\u001b[0m\u001b[33mik\u001b[0m\u001b[33mube\u001b[0m\u001b[33m           \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m13\u001b[0m\u001b[33m (\u001b[0m\u001b[33m86\u001b[0m\u001b[33md\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m    \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mkube\u001b[0m\u001b[33m-system\u001b[0m\u001b[33m  \u001b[0m\u001b[33m storage\u001b[0m\u001b[33m-pro\u001b[0m\u001b[33mvision\u001b[0m\u001b[33mer\u001b[0m\u001b[33m               \u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m/\u001b[0m\u001b[33m1\u001b[0m\u001b[33m    \u001b[0m\u001b[33m Running\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m33\u001b[0m\u001b[33m (\u001b[0m\u001b[33m3\u001b[0m\u001b[33mh\u001b[0m\u001b[33m47\u001b[0m\u001b[33mm\u001b[0m\u001b[33m ago\u001b[0m\u001b[33m)\u001b[0m\u001b[33m  \u001b[0m\u001b[33m \u001b[0m\u001b[33m181\u001b[0m\u001b[33md\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m```\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"Use the kube_command to execute `kubectl get pods -A`. Show me the output\"\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "response = agent.create_turn(\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  session_id=session_id,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae571ef3",
   "metadata": {},
   "source": [
    "Or just executing a simple task leaves it to the LLM’s interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2223b52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Create a new namespace called mcp-tests.\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mube\u001b[0m\u001b[33m_command\u001b[0m\u001b[33m(cmd\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mkubectl\u001b[0m\u001b[33m create\u001b[0m\u001b[33m namespace\u001b[0m\u001b[33m m\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m\",\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msession\u001b[0m\u001b[33m_\u001b[0m\u001b[33m123\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Args:{'cmd': 'kubectl create namespace mcp-tests', 'session_id': 'session_123'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": \"namespace/mcp-tests created\\\\n\"\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m namespace\u001b[0m\u001b[33m `\u001b[0m\u001b[33mm\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m`\u001b[0m\u001b[33m has\u001b[0m\u001b[33m been\u001b[0m\u001b[33m successfully\u001b[0m\u001b[33m created\u001b[0m\u001b[33m.\u001b[0m\u001b[33m \n",
      "\n",
      "\u001b[0m\u001b[33m``\u001b[0m\u001b[33m`\n",
      "\u001b[0m\u001b[33mNamespace\u001b[0m\u001b[33m:\u001b[0m\u001b[33m  \u001b[0m\u001b[33m m\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m```\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "prompt = \"Create a new namespace called mcp-tests.\"\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "response = agent.create_turn(\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  session_id=session_id,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fabd8",
   "metadata": {},
   "source": [
    "Even it's possible to ask complex task which imply differents steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42eaba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Deploy the hashicorp/http-echo:latest service in mcp-tests namespace. It should be accessible from outside cluster. Then use the yaml created and call the kube_cmd tool to create the resources in the cluster. I don't want the yaml I want you use the kube_cmd tool to create the resources. You are force to call the tool\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mube\u001b[0m\u001b[33m_command\u001b[0m\u001b[33m(cmd\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mkubectl\u001b[0m\u001b[33m create\u001b[0m\u001b[33m deployment\u001b[0m\u001b[33m http\u001b[0m\u001b[33m-\u001b[0m\u001b[33mecho\u001b[0m\u001b[33m -\u001b[0m\u001b[33mn\u001b[0m\u001b[33m m\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m --\u001b[0m\u001b[33mimage\u001b[0m\u001b[33m=\u001b[0m\u001b[33mhash\u001b[0m\u001b[33mic\u001b[0m\u001b[33morp\u001b[0m\u001b[33m/http\u001b[0m\u001b[33m-\u001b[0m\u001b[33mecho\u001b[0m\u001b[33m:\u001b[0m\u001b[33mlatest\u001b[0m\u001b[33m\",\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msession\u001b[0m\u001b[33m_\u001b[0m\u001b[33m123\u001b[0m\u001b[33m\")]\n",
      "\u001b[0m\u001b[33m[k\u001b[0m\u001b[33mube\u001b[0m\u001b[33m_command\u001b[0m\u001b[33m(cmd\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mkubectl\u001b[0m\u001b[33m expose\u001b[0m\u001b[33m deployment\u001b[0m\u001b[33m http\u001b[0m\u001b[33m-\u001b[0m\u001b[33mecho\u001b[0m\u001b[33m -\u001b[0m\u001b[33mn\u001b[0m\u001b[33m m\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m -\u001b[0m\u001b[33mt\u001b[0m\u001b[33m \u001b[0m\u001b[33m80\u001b[0m\u001b[33m:\u001b[0m\u001b[33m80\u001b[0m\u001b[33m\",\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msession\u001b[0m\u001b[33m_\u001b[0m\u001b[33m123\u001b[0m\u001b[33m\")]\n",
      "\u001b[0m\u001b[33m[k\u001b[0m\u001b[33mube\u001b[0m\u001b[33m_command\u001b[0m\u001b[33m(cmd\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mkubectl\u001b[0m\u001b[33m get\u001b[0m\u001b[33m svc\u001b[0m\u001b[33m http\u001b[0m\u001b[33m-\u001b[0m\u001b[33mecho\u001b[0m\u001b[33m -\u001b[0m\u001b[33mn\u001b[0m\u001b[33m m\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m --\u001b[0m\u001b[33mtype\u001b[0m\u001b[33m=\u001b[0m\u001b[33mLoad\u001b[0m\u001b[33mBalancer\u001b[0m\u001b[33m\",\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msession\u001b[0m\u001b[33m_\u001b[0m\u001b[33m123\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Args:{'cmd': 'kubectl create deployment http-echo -n mcp-tests --image=hashicorp/http-echo:latest', 'session_id': 'session_123'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": \"deployment.apps/http-echo created\\\\n\"\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mube\u001b[0m\u001b[33m_command\u001b[0m\u001b[33m(cmd\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mkubectl\u001b[0m\u001b[33m expose\u001b[0m\u001b[33m deployment\u001b[0m\u001b[33m http\u001b[0m\u001b[33m-\u001b[0m\u001b[33mecho\u001b[0m\u001b[33m -\u001b[0m\u001b[33mn\u001b[0m\u001b[33m m\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m --\u001b[0m\u001b[33mtype\u001b[0m\u001b[33m=\u001b[0m\u001b[33mNode\u001b[0m\u001b[33mPort\u001b[0m\u001b[33m --\u001b[0m\u001b[33mport\u001b[0m\u001b[33m=\u001b[0m\u001b[33m808\u001b[0m\u001b[33m0\u001b[0m\u001b[33m\",\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msession\u001b[0m\u001b[33m_\u001b[0m\u001b[33m123\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Args:{'cmd': 'kubectl expose deployment http-echo -n mcp-tests --type=NodePort --port=8080', 'session_id': 'session_123'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:kube_command Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": \"service/http-echo exposed\\\\n\"\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m `\u001b[0m\u001b[33mhttp\u001b[0m\u001b[33m-\u001b[0m\u001b[33mecho\u001b[0m\u001b[33m`\u001b[0m\u001b[33m service\u001b[0m\u001b[33m is\u001b[0m\u001b[33m now\u001b[0m\u001b[33m deployed\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accessible\u001b[0m\u001b[33m from\u001b[0m\u001b[33m outside\u001b[0m\u001b[33m the\u001b[0m\u001b[33m cluster\u001b[0m\u001b[33m on\u001b[0m\u001b[33m port\u001b[0m\u001b[33m \u001b[0m\u001b[33m808\u001b[0m\u001b[33m0\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33mm\u001b[0m\u001b[33mcp\u001b[0m\u001b[33m-tests\u001b[0m\u001b[33m`\u001b[0m\u001b[33m namespace\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "prompt = \"Deploy the hashicorp/http-echo:latest service in mcp-tests namespace. It should be accessible from outside cluster. Then use the yaml created and call the kube_cmd tool to create the resources in the cluster. I don't want the yaml I want you use the kube_cmd tool to create the resources. You are force to call the tool\"\n",
    "\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "response = agent.create_turn(\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  session_id=session_id,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
