{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1363d0a4",
   "metadata": {},
   "source": [
    "## Initialize the Agent\n",
    "\n",
    "We are initializing a Llama Stack application which starts a Llama Stack server running alongside our main application. Running a separate Llama Stack server is a better approach for real use cases, but for our simple example this setup is sufficient.\n",
    "\n",
    "On the other hand, we are going to use the model llama3.2:3b-instruct-fp16 served by a local Ollama instance. We need to define a set of rules or instructions so that the LLM can correctly infer when and how to use the tools we provide. We also need to configure the temperature parameter, which controls the balance between strictness and creativity of the model.\n",
    "\n",
    "Finally, we add our tools, which are implemented in the files [math.py](./tools/maths.py) and [uppercase.py](./tools/uppercaser.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e082bd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">ollama</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mollama\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "container_image: null\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "external_providers_dir: null\n",
       "image_name: ollama\n",
       "inference_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">inference_store.db</span>\n",
       "  type: sqlite\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: llama3.<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:3b</span>-instruct-fp16\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "      responses_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">responses_store.db</span>\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">huggingface_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">localfs_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meta_reference_eval.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:11434</span>\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  post_training:\n",
       "  - config:\n",
       "      checkpoint_format: huggingface\n",
       "      device: cpu\n",
       "      distributed_backend: null\n",
       "    provider_id: huggingface\n",
       "    provider_type: inline::huggingface\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: <span style=\"font-weight: bold\">[]</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/jbarea/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  auth: null\n",
       "  host: null\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: <span style=\"font-weight: bold\">[]</span>\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "container_image: null\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "external_providers_dir: null\n",
       "image_name: ollama\n",
       "inference_store:\n",
       "  db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95minference_store.db\u001b[0m\n",
       "  type: sqlite\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: llama3.\u001b[1;92m2:3b\u001b[0m-instruct-fp16\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: ollama\n",
       "  provider_model_id: all-minilm:latest\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "      responses_store:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mresponses_store.db\u001b[0m\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mhuggingface_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mlocalfs_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mmeta_reference_eval.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: \u001b[4;94mhttp://localhost:11434\u001b[0m\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  post_training:\n",
       "  - config:\n",
       "      checkpoint_format: huggingface\n",
       "      device: cpu\n",
       "      distributed_backend: null\n",
       "    provider_id: huggingface\n",
       "    provider_type: inline::huggingface\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: \u001b[32m''\u001b[0m\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/jbarea/.llama/distributions/ollama/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  auth: null\n",
       "  host: null\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "from tools.uppercaser import to_uppercase\n",
    "from tools.maths import add, divide, multiply, subtract\n",
    "\n",
    "\n",
    "model = \"llama3.2:3b-instruct-fp16\"\n",
    "os.environ[\"INFERENCE_MODEL\"] = model\n",
    "client = LlamaStackAsLibraryClient('ollama')\n",
    "client.initialize()\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a smart assistant that helps users with his questions.\n",
    "You have differents available tools and you must use to retrieve information or perform actions.\n",
    "You must not generate code or scripts.\n",
    "No simulate the output, perform all operations with tools available.\n",
    "Multiple tool calls may need to happen serially.\n",
    "You may only call one tool at a time.\n",
    "Remember you may need to pass the output of the first tool to subsequent tool calls in some cases\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "  client,\n",
    "  model=model,\n",
    "  instructions=instructions,\n",
    "  sampling_params={\n",
    "    \"strategy\": {\"type\": \"top_p\", \"temperature\": 0.2, \"top_p\": 0.9},\n",
    "  },\n",
    "  tools=[\n",
    "      to_uppercase, add, divide, multiply, subtract\n",
    "  ],\n",
    ")\n",
    "session_id = agent.create_session('new_session')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d77e6",
   "metadata": {},
   "source": [
    "## Interaction of model with tools\n",
    "\n",
    "Now we can ask the model to perform tasks related to the tools. For example, if the input contains keywords related to the provided tools, the model will infer that it needs to use those specific tools.\n",
    "\n",
    "If we ask to uppercase some text, the presence of the word \"uppercase\" signals that it should use the `to_uppercase` tool. Additionally, if the input involves multiple tasks requiring different tools, the model should call each tool sequentially and use the output of the previous tool as the input for the next one. This is not a default behavior, but we define it explicitly in the model instructions:\n",
    "\"Remember you may need to pass the output of the first tool to subsequent tool calls in some cases.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020f7772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Can you to uppercase 'hello world!'?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[to\u001b[0m\u001b[33m_upper\u001b[0m\u001b[33mcase\u001b[0m\u001b[33m(text\u001b[0m\u001b[33m='\u001b[0m\u001b[33mhello\u001b[0m\u001b[33m world\u001b[0m\u001b[33m!\u001b[0m\u001b[33m')]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m\u001b[32mtool_execution> Tool:to_uppercase Args:{'text': 'hello world!'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:to_uppercase Response:\"HELLO WORLD!\"\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHow\u001b[0m\u001b[33m can\u001b[0m\u001b[33m I\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m you\u001b[0m\u001b[33m further\u001b[0m\u001b[33m?\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"Can you to uppercase 'hello world!'?\"\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "response = agent.create_turn(\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  session_id=session_id,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2223b52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> First, add 3 + 3. Then divide the result by 2. Finally, multiply that result by 6. What is the final answer?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33madd\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m),\u001b[0m\u001b[33m divide\u001b[0m\u001b[33m(result\u001b[0m\u001b[33m=add\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m),\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m2\u001b[0m\u001b[33m),\u001b[0m\u001b[33m multiply\u001b[0m\u001b[33m(result\u001b[0m\u001b[33m=\u001b[0m\u001b[33mdivide\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m),\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m6\u001b[0m\u001b[33m)]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m\u001b[32mtool_execution> Tool:add Args:{'a': 3.0, 'b': 3.0}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:add Response:6\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33mdivide\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m6\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m2\u001b[0m\u001b[33m)]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:divide Args:{'a': 6.0, 'b': 2.0}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:divide Response:3.0\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[m\u001b[0m\u001b[33multiply\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m0\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m6\u001b[0m\u001b[33m)]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:multiply Args:{'a': 3.0, 'b': 6.0}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:multiply Response:18.0\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m final\u001b[0m\u001b[33m answer\u001b[0m\u001b[33m is\u001b[0m\u001b[33m \u001b[0m\u001b[33m18\u001b[0m\u001b[33m.\u001b[0m\u001b[33m0\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "prompt = \"First, add 3 + 3. Then divide the result by 2. Finally, multiply that result by 6. What is the final answer?\"\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "response = agent.create_turn(\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  session_id=session_id,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fabd8",
   "metadata": {},
   "source": [
    "## Using MCP server to provide tools to Agent\n",
    "\n",
    "\n",
    "Now we will create a new agent that uses an MCP server to register tools. These tools are the same as the ones we used in the previous section, but this time they are provided via the MCP server.\n",
    "\n",
    "Using an MCP server allows serving tools over HTTP to connected clients. This enables centralization of tools alongside different AI applications.\n",
    "\n",
    "To do this, we need to run our MCP server, which is implemented with FastMCP and can be found in [mcp_server](./mcp_server.py). To start the server, simply run: `python mcp_server.py`\n",
    "\n",
    "When you execute the script below, you will see the tools registered—the default tools and those served by the MCP server. Finally, we can query the model with the same prompt as before, and the output should be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42eaba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(description='Insert documents into memory', identifier='insert_into_memory', parameters=[], provider_id='rag-runtime', tool_host=None, toolgroup_id='builtin::rag', type='tool', metadata=None, provider_resource_id=None),\n",
      " Tool(description='Search for information in a database.', identifier='knowledge_search', parameters=[Parameter(description='The query to search for. Can be a natural language sentence or keywords.', name='query', parameter_type='string', required=True, default=None)], provider_id='rag-runtime', tool_host=None, toolgroup_id='builtin::rag', type='tool', metadata=None, provider_resource_id=None),\n",
      " Tool(description='Search the web for information', identifier='web_search', parameters=[Parameter(description='The query to search for', name='query', parameter_type='string', required=True, default=None)], provider_id='tavily-search', tool_host=None, toolgroup_id='builtin::websearch', type='tool', metadata=None, provider_resource_id=None),\n",
      " Tool(description='Query WolframAlpha for computational knowledge', identifier='wolfram_alpha', parameters=[Parameter(description='The query to compute', name='query', parameter_type='string', required=True, default=None)], provider_id='wolfram-alpha', tool_host=None, toolgroup_id='builtin::wolfram_alpha', type='tool', metadata=None, provider_resource_id=None),\n",
      " Tool(description='\\n    Returns the sum of a and b.\\n\\n    :param a: The first number (int or float).\\n    :param b: The second number (int or float).\\n    :returns: The result of a + b.\\n    ', identifier='call_add', parameters=[Parameter(description='', name='a', parameter_type='integer', required=True, default=None), Parameter(description='', name='b', parameter_type='integer', required=True, default=None), Parameter(description='', name='session_id', parameter_type='string', required=True, default=None)], provider_id='model-context-protocol', tool_host=None, toolgroup_id='mcp::custom', type='tool', metadata={'endpoint': 'http://127.0.0.1:5000/sse'}, provider_resource_id=None),\n",
      " Tool(description='\\n    Returns the result of subtracting b from a.\\n\\n    :param a: The number to subtract from (int or float).\\n    :param b: The number to subtract (int or float).\\n    :returns: The result of a - b.\\n    ', identifier='call_subtract', parameters=[Parameter(description='', name='a', parameter_type='integer', required=True, default=None), Parameter(description='', name='b', parameter_type='integer', required=True, default=None), Parameter(description='', name='session_id', parameter_type='string', required=True, default=None)], provider_id='model-context-protocol', tool_host=None, toolgroup_id='mcp::custom', type='tool', metadata={'endpoint': 'http://127.0.0.1:5000/sse'}, provider_resource_id=None),\n",
      " Tool(description='\\n    Returns the product of a and b.\\n\\n    :param a: The first factor (int or float).\\n    :param b: The second factor (int or float).\\n    :returns: The result of a * b.\\n    ', identifier='call_multiply', parameters=[Parameter(description='', name='a', parameter_type='integer', required=True, default=None), Parameter(description='', name='b', parameter_type='integer', required=True, default=None), Parameter(description='', name='session_id', parameter_type='string', required=True, default=None)], provider_id='model-context-protocol', tool_host=None, toolgroup_id='mcp::custom', type='tool', metadata={'endpoint': 'http://127.0.0.1:5000/sse'}, provider_resource_id=None),\n",
      " Tool(description='\\n    Returns the result of dividing a by b.\\n\\n    :param a: The numerator (int or float).\\n    :param b: The denominator (int or float).\\n    :returns: The result of a / b.\\n    :raises ValueError: If b is zero.\\n    ', identifier='call_divide', parameters=[Parameter(description='', name='a', parameter_type='integer', required=True, default=None), Parameter(description='', name='b', parameter_type='integer', required=True, default=None), Parameter(description='', name='session_id', parameter_type='string', required=True, default=None)], provider_id='model-context-protocol', tool_host=None, toolgroup_id='mcp::custom', type='tool', metadata={'endpoint': 'http://127.0.0.1:5000/sse'}, provider_resource_id=None),\n",
      " Tool(description='\\n    Converts a string to uppercase.\\n\\n    :param text: The input string to convert.\\n    :returns: The uppercase version of the input string.\\n    ', identifier='call_to_uppercase', parameters=[Parameter(description='', name='text', parameter_type='string', required=True, default=None), Parameter(description='', name='session_id', parameter_type='string', required=True, default=None)], provider_id='model-context-protocol', tool_host=None, toolgroup_id='mcp::custom', type='tool', metadata={'endpoint': 'http://127.0.0.1:5000/sse'}, provider_resource_id=None),\n",
      " Tool(description='Executing a command on Kubernetes and return the output\\n    :param cmd: The command to be executed\\n    :returns: The output of the command executed\\n    ', identifier='kube_command', parameters=[Parameter(description='', name='cmd', parameter_type='string', required=True, default=None), Parameter(description='', name='session_id', parameter_type='string', required=True, default=None)], provider_id='model-context-protocol', tool_host=None, toolgroup_id='mcp::custom', type='tool', metadata={'endpoint': 'http://127.0.0.1:5000/sse'}, provider_resource_id=None)]\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.types.toolgroup_register_params import McpEndpoint\n",
    "from pprint import pprint\n",
    "\n",
    "client.toolgroups.register(\n",
    "    toolgroup_id=\"mcp::custom\",\n",
    "    provider_id=\"model-context-protocol\",\n",
    "    mcp_endpoint=McpEndpoint(uri=\"http://127.0.0.1:5000/sse\"),\n",
    ")\n",
    "\n",
    "mcp_agent = Agent(\n",
    "  client,\n",
    "  model=model,\n",
    "  instructions=instructions,\n",
    "  tools=[\"mcp::custom\"],\n",
    ")\n",
    "\n",
    "\n",
    "tools = client.tools.list()\n",
    "pprint(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec71fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Can you to uppercase 'hello world!'?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33mcall\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_upper\u001b[0m\u001b[33mcase\u001b[0m\u001b[33m(text\u001b[0m\u001b[33m='\u001b[0m\u001b[33mhello\u001b[0m\u001b[33m world\u001b[0m\u001b[33m!\u001b[0m\u001b[33m')]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_to_uppercase Args:{'text': 'hello world!'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_to_uppercase Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": \"HELLO WORLD!\"\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHow\u001b[0m\u001b[33m can\u001b[0m\u001b[33m I\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m you\u001b[0m\u001b[33m further\u001b[0m\u001b[33m?\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0mprompt> First, add 3 + 3. Then divide the result by 2. Finally, multiply that result by 6. What is the final answer?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33mcall\u001b[0m\u001b[33m_add\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m,\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33m123\u001b[0m\u001b[33m4\u001b[0m\u001b[33m\"),\u001b[0m\u001b[33m call\u001b[0m\u001b[33m_div\u001b[0m\u001b[33mide\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m9\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m2\u001b[0m\u001b[33m,\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33m123\u001b[0m\u001b[33m4\u001b[0m\u001b[33m\"),\u001b[0m\u001b[33m call\u001b[0m\u001b[33m_multiply\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m54\u001b[0m\u001b[33m,\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33m123\u001b[0m\u001b[33m4\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_add Args:{'a': 3.0, 'b': 3.0, 'session_id': '1234'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_add Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": 6\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33mcall\u001b[0m\u001b[33m_div\u001b[0m\u001b[33mide\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m6\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m2\u001b[0m\u001b[33m,\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33m123\u001b[0m\u001b[33m4\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_divide Args:{'a': 6.0, 'b': 2.0, 'session_id': '1234'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_divide Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": 3.0\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33mcall\u001b[0m\u001b[33m_multiply\u001b[0m\u001b[33m(a\u001b[0m\u001b[33m=\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m0\u001b[0m\u001b[33m,\u001b[0m\u001b[33m b\u001b[0m\u001b[33m=\u001b[0m\u001b[33m6\u001b[0m\u001b[33m,\u001b[0m\u001b[33m session\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33m123\u001b[0m\u001b[33m4\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_multiply Args:{'a': 3.0, 'b': 6.0, 'session_id': '1234'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:call_multiply Response:[TextContentItem(text='{\\n  \"success\": true,\\n  \"result\": 18\\n}', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m final\u001b[0m\u001b[33m answer\u001b[0m\u001b[33m is\u001b[0m\u001b[33m \u001b[0m\u001b[33m18\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Can you to uppercase 'hello world!'?\",\n",
    "    \"First, add 3 + 3. Then divide the result by 2. Finally, multiply that result by 6. What is the final answer?\",\n",
    "]\n",
    "\n",
    "mcp_session_id = mcp_agent.create_session('new_session')\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = mcp_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=mcp_session_id,\n",
    "    )\n",
    "\n",
    "    print(\"prompt>\", prompt)\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
