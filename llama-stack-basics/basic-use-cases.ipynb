{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454ff27e",
   "metadata": {},
   "source": [
    "# Basic Use Cases\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "INFERENCE_MODEL=llama3.2:3b-instruct-fp16 llama stack build --template ollama --image-type venv --image-name venv --run\n",
    "```\n",
    "\n",
    "* You can use your own virtual environment, but in this case, we recommend using the one that Llama Stack will create for you.\n",
    "\n",
    "You should see output like the following:\n",
    "\n",
    "```\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "\n",
    "### (Optional) Use a Container Instead of a Local Process\n",
    "\n",
    "Set environment variables:\n",
    "\n",
    "```bash\n",
    "export INFERENCE_MODEL=\"llama3.2:3b\"\n",
    "export LLAMA_STACK_PORT=8321\n",
    "mkdir -p ~/.llama\n",
    "```\n",
    "\n",
    "Start the server using Docker or Podman:\n",
    "\n",
    "```bash\n",
    "podman run --privileged -it \\\n",
    "  --pull always \\\n",
    "  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n",
    "  -v ~/.llama:/root/.llama \\\n",
    "  --network=host \\\n",
    "  llamastack/distribution-ollama \\\n",
    "  --port $LLAMA_STACK_PORT \\\n",
    "  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n",
    "  --env OLLAMA_URL=http://localhost:11434\n",
    "```\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Go through the different cases and execute the Jupyter cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71aed61",
   "metadata": {},
   "source": [
    "### Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Select the first LLM\n",
    "llm = next(m for m in models if m.model_type == \"llm\")\n",
    "model_id = llm.identifier\n",
    "\n",
    "print(\"Model:\", model_id)\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me something funny\"},\n",
    "    ],\n",
    ")\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295713f6",
   "metadata": {},
   "source": [
    "### Basic Agent usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "client = LlamaStackClient(base_url=f\"http://localhost:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "llm = next(m for m in models if m.model_type == \"llm\")\n",
    "model_id = llm.identifier\n",
    "\n",
    "agent = Agent(client, model=model_id, instructions=\"You are a helpful assistant.\")\n",
    "\n",
    "s_id = agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "print(\"Streaming response...\")\n",
    "stream = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Give me a short technical overview of LLM\"}], session_id=s_id, stream=True\n",
    ")\n",
    "for event in AgentEventLogger().log(stream):\n",
    "    event.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5010a1a",
   "metadata": {},
   "source": [
    "### Basic RAG usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack_client.types import Document\n",
    "import uuid\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "# Create a vector database instance\n",
    "embed_lm = next(m for m in client.models.list() if m.model_type == \"embedding\")\n",
    "embedding_model = embed_lm.identifier\n",
    "vector_db_id = f\"v{uuid.uuid4().hex}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "\n",
    "# Create Documents\n",
    "urls = [\n",
    "    \"memory_optimizations.rst\",\n",
    "    \"chat.rst\",\n",
    "    \"llama3.rst\",\n",
    "    \"qat_finetune.rst\",\n",
    "    \"lora_finetune.rst\",\n",
    "]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "\n",
    "# Insert documents\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")\n",
    "\n",
    "# Get the model being served\n",
    "llm = next(m for m in client.models.list() if m.model_type == \"llm\")\n",
    "model = llm.identifier\n",
    "\n",
    "# Create the RAG agent\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=model,\n",
    "    instructions=\"You are a helpful assistant. Use the RAG tool to answer questions as needed.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "turns = [\"what is torchtune\", \"tell me about dora\"]\n",
    "\n",
    "for t in turns:\n",
    "    print(\"user>\", t)\n",
    "    stream = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": t}], session_id=session_id, stream=True\n",
    "    )\n",
    "    for event in AgentEventLogger().log(stream):\n",
    "        event.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37436b",
   "metadata": {},
   "source": [
    "### Web search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0506ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=f\"http://localhost:8321\",\n",
    "    provider_data={\n",
    "        \"tavily_search_api_key\": \"xxxx\"\n",
    "    },  # Set this from the client side. No need to provide it if it has already been configured on the Llama Stack server.\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    instructions=(\n",
    "        \"You are a web search assistant, must use websearch tool to look up the most current and precise information available. \"\n",
    "    ),\n",
    "    tools=[\"builtin::websearch\"],\n",
    ")\n",
    "\n",
    "session_id = agent.create_session(\"websearch-session\")\n",
    "\n",
    "response = agent.create_turn(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How did the USA perform in the last Olympics?\"}\n",
    "    ],\n",
    "    session_id=session_id,\n",
    ")\n",
    "for log in EventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9c7d8",
   "metadata": {},
   "source": [
    "### Code interpreter tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    client,\n",
    "    instructions=\"\"\"\n",
    "    You are a highly reliable, concise, and precise assistant.\n",
    "    Always show the generated code, never generate your own code, and never anticipate results.\n",
    "    \"\"\",\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    tools=[\"builtin::code_interpreter\"],\n",
    ")\n",
    "\n",
    "session_id = agent.create_session(\"tool_session\")\n",
    "\n",
    "response = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Can you generate code to say Hello World in C++ and then execute?\"}],\n",
    "    session_id=session_id,\n",
    ")\n",
    "\n",
    "for log in EventLogger().log(response):\n",
    "    log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
