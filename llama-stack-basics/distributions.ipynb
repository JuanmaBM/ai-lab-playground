{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b30b57f",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "\n",
    "If you don’t need Llama Stack Server or it’s not available in your environment, you can keep working with Llama Stack capabilities. This approach allows you to directly utilize Llama Stack features without the overhead of managing a separate server process.\n",
    "\n",
    " A distribution is a pre-packaged set of Llama Stack components configured to work out of the box. You can use different default distributions as ollama, tgi or remote-vllm or create your own specifically for your needs.\n",
    "\n",
    "For instance, when utilizing an inference service provided by vLLM, you may prefer it over a local Ollama instance. In such cases, configure Llama Stack to use the specific inference service by employing \"remote-vllm\" distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ee3b0",
   "metadata": {},
   "source": [
    "# Ollama Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6963c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack_client import Agent\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "os.environ[\"INFERENCE_MODEL\"] = model\n",
    "client = LlamaStackAsLibraryClient('ollama')\n",
    "client.initialize()\n",
    "agent = Agent(\n",
    "  client,\n",
    "  model=model,\n",
    "  instructions=\"You are an helpful agent\",\n",
    ")\n",
    "\n",
    "response = agent.create_turn(\n",
    "  messages=[{\"role\": \"user\", \"content\": \"say Hi!\"}],\n",
    "  stream=False,\n",
    "  session_id=agent.create_session('new_session'),\n",
    ")\n",
    "print(response.output_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bc756",
   "metadata": {},
   "source": [
    "# Remote VLLM Ditribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack_client import Agent\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "model = \"mistral-7b-instruct\"\n",
    "os.environ[\"INFERENCE_MODEL\"] = model\n",
    "os.environ[\"VLLM_API_TOKEN\"] = \"xxxx\"\n",
    "os.environ[\"VLLM_URL\"] = \"https://mistral-7b-instruct-v0-3-maas-services.com:443/v1\"\n",
    "client = LlamaStackAsLibraryClient('remote-vllm')\n",
    "client.initialize()\n",
    "agent = Agent(\n",
    "  client,\n",
    "  model=model,\n",
    "  instructions=\"You are an helpful agent\",\n",
    ")\n",
    "\n",
    "response = agent.create_turn(\n",
    "  messages=[{\"role\": \"user\", \"content\": \"say Hi!\"}],\n",
    "  stream=False,\n",
    "  session_id=agent.create_session('new_session'),\n",
    ")\n",
    "print(response.output_message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
