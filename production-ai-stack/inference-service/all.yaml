---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "test"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "test"
  template:
    metadata:
      labels:
        app: "test"
    spec:
      containers:
        - name: "test"
          image: quay.io/jbarea/llama-stack-sample-app:1.2
          ports:
            - containerPort: 8000
          resources:
            requests:
              memory: "250Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          env:
            - name: SUMMARY_PROMPT
              value: "You are a helpful assistant."
            - name: LLM_MODEL_NAME
              value: "llama-scout"
            - name: LLAMA_STACK_SERVER_URL
              value: "http://llama-stack-server:8321"
          
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 2
          
          imagePullPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: "test"
spec:
  selector:
    app: "test"
  ports:
    - protocol: TCP
      port: 443
      targetPort: 8000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test
  annotations:
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "1200"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "1200"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "1200"
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: test
                port:
                  number: 8000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack-server
  template:
    metadata:
      labels:
        app: llama-stack-server
    spec:
      containers:
        - name: llama-stack-server
          image: llamastack/distribution-starter:0.2.20
          ports:
            - containerPort: 8321
          resources:
            limits:
              cpu: 700m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          env:
            - name: VLLM_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: "test-secret"
                  key: INFERENCE_API_KEY
            - name: VLLM_URL
              value: "https://mistral-small-24b-w8a8-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
          imagePullPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack-server
spec:
  selector:
    app: llama-stack-server
  ports:
    - protocol: TCP
      port: 8321
      targetPort: 8321
---
apiVersion: v1
kind: Secret
metadata:
  name: "test-secret"
type: Opaque
data:
  INFERENCE_API_KEY: ZTM3Njg4M2YzY2RkMDkwMWI0ZTA1OGZhMzk2ZGIzZjQ=



